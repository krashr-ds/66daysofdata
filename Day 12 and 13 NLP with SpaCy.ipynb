{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "abaca815",
   "metadata": {},
   "source": [
    "## Advanced NLP with SpaCy\n",
    "\n",
    "DataCamp: 6/16 & 6/17/2022\n",
    "\n",
    "KPR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7b1039ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "# spacy.cli.download(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "be9148df",
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.lang.en import English\n",
    "nlp = English()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f1a1cee8",
   "metadata": {},
   "outputs": [],
   "source": [
    "hound =  open('Datasets/hound.txt','r', encoding=\"utf-8\")\n",
    "hound_text = hound.read()\n",
    "hound.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "28359a8d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Holmes'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc = nlp(hound_text)\n",
    "token1 = doc[3]\n",
    "token1.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "482f0d59",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Holmes, who was usually very late in the mornings, save upon those not infrequent occasions when he was up all night, was seated at the breakfast table."
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "span1 = doc[3:34]\n",
    "span1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a91dc701",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indexes:  [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30]\n",
      "Text:  ['Holmes', ',', 'who', 'was', 'usually', 'very', 'late', 'in', 'the', 'mornings', ',', 'save', 'upon', 'those', 'not', 'infrequent', 'occasions', 'when', 'he', 'was', 'up', 'all', 'night', ',', 'was', 'seated', 'at', 'the', 'breakfast', 'table', '.']\n",
      "Is Alpha?:  [True, False, True, True, True, True, True, True, True, True, False, True, True, True, True, True, True, True, True, True, True, True, True, False, True, True, True, True, True, True, False]\n",
      "Is Punctuation?:  [False, True, False, False, False, False, False, False, False, False, True, False, False, False, False, False, False, False, False, False, False, False, False, True, False, False, False, False, False, False, True]\n",
      "Like Num?:  [False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False]\n"
     ]
    }
   ],
   "source": [
    "subdoc = nlp(span1.text)\n",
    "print(\"Indexes: \", [token.i for token in subdoc])\n",
    "print(\"Text: \", [token.text for token in subdoc])\n",
    "print(\"Is Alpha?: \", [token.is_alpha for token in subdoc])\n",
    "print(\"Is Punctuation?: \", [token.is_punct for token in subdoc])\n",
    "print(\"Like Num?: \", [token.like_num for token in subdoc])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c04615af",
   "metadata": {},
   "source": [
    "### Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3f3ddd2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apple is looking at buying U.K. startup for $1 billion\n",
      "Apple PROPN nsubj\n",
      "is AUX aux\n",
      "looking VERB ROOT\n",
      "at ADP prep\n",
      "buying VERB pcomp\n",
      "U.K. PROPN compound\n",
      "startup NOUN dobj\n",
      "for ADP prep\n",
      "$ SYM quantmod\n",
      "1 NUM compound\n",
      "billion NUM pobj\n"
     ]
    }
   ],
   "source": [
    "from spacy.lang.en.examples import sentences \n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp(sentences[0])\n",
    "print(doc.text)\n",
    "for token in doc:\n",
    "    print(token.text, token.pos_, token.dep_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "104cae91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apple ORG\n",
      "U.K. GPE\n",
      "$1 billion MONEY\n"
     ]
    }
   ],
   "source": [
    "# Iterate over the predicted entities\n",
    "for ent in doc.ents:\n",
    "    # print the entity text and its label\n",
    "    print(ent.text, ent.label_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2655b28d",
   "metadata": {},
   "source": [
    "Subsetting a missing term: IPhone X\n",
    "\n",
    "Running the model on the sample sentence below failed to pick up the term 'IPhone X'; one way to get at this term is to pick it up manually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "72f28fa4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New iPhone X release date leaked as Apple reveals pre-orders by mistake\n",
      "New PROPN amod\n",
      "iPhone PROPN compound\n",
      "X NOUN compound\n",
      "release NOUN compound\n",
      "date NOUN ROOT\n",
      "leaked VERB acl\n",
      "as SCONJ mark\n",
      "Apple PROPN nsubj\n",
      "reveals VERB advcl\n",
      "pre ADJ dobj\n",
      "- NOUN dobj\n",
      "orders NOUN dobj\n",
      "by ADP prep\n",
      "mistake NOUN pobj\n"
     ]
    }
   ],
   "source": [
    "doc2 = nlp(\"New iPhone X release date leaked as Apple reveals pre-orders by mistake\")\n",
    "print(doc2.text)\n",
    "for token in doc2:\n",
    "    print(token.text, token.pos_, token.dep_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2f8e8f8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apple ORG\n",
      "Missing entity: iPhone X\n"
     ]
    }
   ],
   "source": [
    "for ent in doc2.ents:\n",
    "    # print the entity text and label\n",
    "    print(ent.text, ent.label_)\n",
    "\n",
    "# Get the span for \"iPhone X\"\n",
    "iphone_x = doc2[1:3]\n",
    "\n",
    "# Print the span text\n",
    "print('Missing entity:', iphone_x.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84367443",
   "metadata": {},
   "source": [
    "### Rules-based Matching\n",
    "\n",
    "- Matching on document objects, not only strings as with regular expressions\n",
    "- Match on tokens or token attributes\n",
    "- Use the model's predictions\n",
    "- Match on a word only if it is a desired part of speech; eg. 'duck' as a verb, not a noun\n",
    "\n",
    "### Match Patterns\n",
    "\n",
    "- A list of dictionaries, one per token\n",
    "- Matching the exact token text: e.g. [{'ORTH': 'iPhone'}, {'ORTH': 'X'}]\n",
    "- Matching the lexical attributes: e.g. [{'LOWER': 'iphone'}, {'LOWER': 'x'}]\n",
    "- Matching token attributes: e.g. [{'LEMMA': 'buy'}, {'POS': 'NOUN'}] <- would match 'buying milk' or 'bought flowers'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "db7a77be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "[[{'ORTH': 'iPhone'}, {'ORTH': 'X'}]]\n"
     ]
    }
   ],
   "source": [
    "from spacy.matcher import Matcher\n",
    "\n",
    "match_model = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# init matcher with shared vocabulary\n",
    "matcher = Matcher(match_model.vocab)\n",
    "\n",
    "# add a test pattern (Note: this is different syntax from the DataCamp course, which is based on an old version of the library.)\n",
    "pattern = [[{'ORTH': 'iPhone'}, {'ORTH': 'X'}]]\n",
    "matcher.add('IPHONE_PATTERN', pattern)\n",
    "on_match, patterns = matcher.get(\"IPHONE_PATTERN\")\n",
    "\n",
    "print(on_match)\n",
    "print(patterns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "258b8f0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New iPhone X release date leaked as Apple reveals pre-orders by mistake\n",
      "\n",
      "Matches: ['iPhone X']\n"
     ]
    }
   ],
   "source": [
    "print(doc2.text)\n",
    "print()\n",
    "matches = matcher(doc2)\n",
    "\n",
    "print('Matches:', [doc2[start:end].text for match_id, start, end in matches])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb3b30a1",
   "metadata": {},
   "source": [
    "### Operators and quantifiers\n",
    "\n",
    "- OP: ! // match NOT the token\n",
    "- OP: ? // match 0 or 1 times\n",
    "- OP: + // match 1 or more times\n",
    "- OP: * // match 0 or more times\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4e53f4e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[{'LEMMA': 'buy'}, {'POS': 'DET', 'OP': '?'}, {'POS': 'NOUN'}]]\n",
      "Matches: ['bought a smartphone', 'buying apps']\n"
     ]
    }
   ],
   "source": [
    "p2 = [[{'LEMMA': 'buy'},\n",
    "       {'POS': 'DET', 'OP': '?'}, # match 0 or 1 times\n",
    "       {'POS': 'NOUN'}]]\n",
    "\n",
    "matcher.add('PURCHASES', p2)\n",
    "_, patterns = matcher.get(\"PURCHASES\")\n",
    "\n",
    "print(patterns)\n",
    "\n",
    "example = nlp(\"I bought a smartphone, now I'm buying apps.\")\n",
    "m2 = matcher(example)\n",
    "\n",
    "# print matches using list-comprehension\n",
    "print('Matches:', [example[start:end].text for match_id, start, end in m2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82c25240",
   "metadata": {},
   "source": [
    "Matching digits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "37d311c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total matches found: 3\n",
      "Match found: iOS 7\n",
      "Match found: iOS 11\n",
      "Match found: iOS 10\n"
     ]
    }
   ],
   "source": [
    "ios_text = nlp(\"After making the iOS update you won't notice a radical system-wide redesign: nothing like the aesthetic upheaval we got with iOS 7. Most of iOS 11's furniture remains the same as in iOS 10. But you will discover some tweaks once you delve a little deeper.\")\n",
    "\n",
    "# Write a pattern for full iOS versions (\"iOS 7\", \"iOS 11\", \"iOS 10\")\n",
    "ios_pattern = [[{'TEXT': 'iOS'}, {'IS_DIGIT': True}]]\n",
    "\n",
    "# Add the pattern to the matcher and apply the matcher to the doc\n",
    "matcher.add('IOS_VERSION_PATTERN', ios_pattern)\n",
    "ios_matches = matcher(ios_text)\n",
    "print('Total matches found:', len(ios_matches))\n",
    "\n",
    "# Iterate over the matches and print the span text\n",
    "for match_id, start, end in ios_matches:\n",
    "    print('Match found:', ios_text[start:end].text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2c3d8fb",
   "metadata": {},
   "source": [
    "Matching an \"adjective, noun, optional noun\" pattern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9c454f17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total matches found: 5\n",
      "Match found: beautiful design\n",
      "Match found: smart search\n",
      "Match found: automatic labels\n",
      "Match found: optional voice\n",
      "Match found: optional voice responses\n"
     ]
    }
   ],
   "source": [
    "anydoc = nlp(\"Features of the app include a beautiful design, smart search, automatic labels and optional voice responses.\")\n",
    "\n",
    "# Write a pattern for adjective plus one or two nouns\n",
    "ann_pattern = [[{'POS': 'ADJ'}, {'POS': 'NOUN'}, {'POS': 'NOUN', 'OP': '?'}]]\n",
    "\n",
    "# Add the pattern to the matcher and apply the matcher to the doc\n",
    "matcher.add('ADJ_NOUN_PATTERN', ann_pattern)\n",
    "ms = matcher(anydoc)\n",
    "print('Total matches found:', len(ms))\n",
    "\n",
    "# Iterate over the matches and print the span text\n",
    "for match_id, start, end in ms:\n",
    "    print('Match found:', anydoc[start:end].text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0264a7c",
   "metadata": {},
   "source": [
    "### Vocabulary, Lexemes and StringStore "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "637dd0a3",
   "metadata": {},
   "source": [
    "A \"lexeme\" is an entry in the vocabulary; see an example below that shows a lexeme, its text, orth (hash), an some attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3e4860dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "coffee 3197928453018144401 True 3197928453018144401 13110060611322374290 3197928453018144401 False False\n",
      "chickens 8787911050361940795 True 8787911050361940795 13110060611322374290 8787911050361940795 False False\n"
     ]
    }
   ],
   "source": [
    "weird = nlp(\"I love coffee and chickens.\")\n",
    "coffee = nlp.vocab['coffee']\n",
    "chickens = nlp.vocab['chickens']\n",
    "\n",
    "print(coffee.text, coffee.orth, coffee.is_alpha, coffee.norm, coffee.shape, coffee.lower, coffee.is_title, coffee.is_currency)\n",
    "print(chickens.text, chickens.orth, chickens.is_alpha, chickens.norm, chickens.shape, chickens.lower, chickens.is_title, chickens.is_currency)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "67796bdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✔ Download and installation successful\n",
      "You can now load the package via spacy.load('en_core_web_md')\n"
     ]
    }
   ],
   "source": [
    "spacy.cli.download(\"en_core_web_md\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "eafe1a96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.45024433732032776\n",
      "0.9204466307952975\n"
     ]
    }
   ],
   "source": [
    "md = spacy.load(\"en_core_web_md\")\n",
    "gloomy = md(\"I am so angry; this is horrible.\")\n",
    "happy = md(\"I love you; it is wonderful to see you.\")\n",
    "\n",
    "horrible = md.vocab['horrible']\n",
    "wonderful = md.vocab['wonderful']\n",
    "\n",
    "print(horrible.similarity(wonderful))\n",
    "print(gloomy.similarity(happy))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e29eba6c",
   "metadata": {},
   "source": [
    "While it appears that the model doesn't view the adjectives 'horrible' and 'wonderful' as similar, it DOES seem to view the two sentences as similar overall.  Perhaps because they both express strong emotions?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ab32f275",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6588226006975084\n"
     ]
    }
   ],
   "source": [
    "scientific = md(\"The fit of many models can be improved with regularization techniques.\")\n",
    "slang = md(\"Yo, dude! This is the gnarliest trail I've been on in a while!\")\n",
    "\n",
    "print(scientific.similarity(slang))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41948de5",
   "metadata": {},
   "source": [
    "I'm surprised these two sentences were deemed that similar."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6635a0b5",
   "metadata": {},
   "source": [
    "### Manual creation of Docs and Spans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "8335f2e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No Worries!\n",
      "No Worries\n",
      "EXCLAMATION\n",
      "15559262463737172580\n"
     ]
    }
   ],
   "source": [
    "# Probably not something to use too often\n",
    "\n",
    "en = English()\n",
    "from spacy.tokens import Doc, Span\n",
    "\n",
    "words = ['No', 'Worries', '!']\n",
    "spaces = [True, False, False]\n",
    "aussie_ex = Doc(md.vocab, words=words, spaces=spaces)\n",
    "span = Span(aussie_ex, 0, 2, label=\"EXCLAMATION\")\n",
    "\n",
    "print(aussie_ex)\n",
    "print(span)\n",
    "print(span.label_)\n",
    "print(aussie_ex[1].orth)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78fb083e",
   "metadata": {},
   "source": [
    "### More Word and Phrase Matching\n",
    "\n",
    "Use pattern1 to match all case-insensitive mentions of \"Amazon\" plus a title-cased proper noun.\n",
    "\n",
    "Use pattern2 to match all case-insensitive mentions of \"ad-free\", plus the following noun."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "8c65c0f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PATTERN1 Amazon Prime\n",
      "PATTERN2 ad-free viewing\n",
      "PATTERN1 Amazon Prime\n",
      "PATTERN2 ad-free viewing\n",
      "PATTERN2 ad-free viewing\n",
      "PATTERN2 ad-free viewing\n"
     ]
    }
   ],
   "source": [
    "ad_free_amazon = 'Twitch Prime, the perks program for Amazon Prime members offering free loot, games and other benefits, is ditching one of its best features: ad-free viewing. According to an email sent out to Amazon Prime members today, ad-free viewing will no longer be included as a part of Twitch Prime for new members, beginning on September 14. However, members with existing annual subscriptions will be able to continue to enjoy ad-free viewing until their subscription comes up for renewal. Those with monthly subscriptions will have access to ad-free viewing until October 15.'\n",
    "phrase = nlp(ad_free_amazon)\n",
    "\n",
    "# Create the match patterns\n",
    "pattern1 = [[{'LOWER': 'amazon'}, {'IS_TITLE': True, 'POS': 'PROPN'}]]\n",
    "pattern2 = [[{'LOWER': 'ad'}, {'TEXT': '-'}, {'LOWER': 'free'}, {'POS': 'NOUN'}]]\n",
    "\n",
    "# Initialize the Matcher and add the patterns\n",
    "matcher = Matcher(nlp.vocab)\n",
    "matcher.add('PATTERN1', pattern1)\n",
    "matcher.add('PATTERN2', pattern2)\n",
    "\n",
    "# Iterate over the matches\n",
    "for match_id, start, end in matcher(phrase):\n",
    "    # Print pattern string name and text of matched span\n",
    "    print(phrase.vocab.strings[match_id], phrase[start:end].text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee5a5e6e",
   "metadata": {},
   "source": [
    "This was hard because at first, I did not understand that you need a FULL DICT entry for each ITEM you want to match.  I kept getting confused, thinking 'Oh, this is a phrase, so the whole phrase should be within the same set of curly braces'...\n",
    "\n",
    "NO!  {'LOWER', 'each'}, {'LOWER', 'item'}, {'LOWER', 'needs'}, {'TEXT', 'a'}, {'LOWER', 'match'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "c71828b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "APATTERN very silly\n",
      "APATTERN very very silly\n",
      "APATTERN very very very silly\n"
     ]
    }
   ],
   "source": [
    "short = nlp(\"This thing is very very very silly.\")\n",
    "apattern = [[{'TEXT': 'very', 'OP': '+'}, {'LOWER': 'silly'}]]\n",
    "\n",
    "m = Matcher(nlp.vocab)\n",
    "m.add('APATTERN', apattern)\n",
    "\n",
    "for match_id, start, end in m(short):\n",
    "    # Print pattern string name and text of matched span\n",
    "    print(short.vocab.strings[match_id], short[start:end].text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4490b258",
   "metadata": {},
   "source": [
    "This is not easy to wrap my head around.  In a regular expression, this would match ONCE - 'very very very silly', not three separate times..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e75010b9",
   "metadata": {},
   "source": [
    "### Using the Phrase Matcher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "9f39cfc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[war in Ukraine]\n"
     ]
    }
   ],
   "source": [
    "from spacy.matcher import PhraseMatcher\n",
    "\n",
    "s = \"Russia is denying its role in the war in Ukraine\"\n",
    "sdoc = nlp(s)\n",
    "\n",
    "pm = PhraseMatcher(nlp.vocab)\n",
    "pm.add(\"WAR\", [nlp(\"war in Ukraine\")], on_match=on_match)\n",
    "\n",
    "# Call the matcher on the test document and print the result\n",
    "matches = pm(sdoc)\n",
    "print([sdoc[start:end] for match_id, start, end in matches])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87ca5f74",
   "metadata": {},
   "source": [
    "### Custom Component Example\n",
    "\n",
    "NOTES:\n",
    "\n",
    "This doesn't work, because I didn't actually create an animal phrase matcher (TO DO)\n",
    "\n",
    "The code that came from DataCamp had to be modified using the 'Language Processing Pipelines' reference in the spaCy documentation; a lot of things have changed in the new version of spaCy and the DataCamp course has not kept up with these changes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "ddf836fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "from spacy.language import Language\n",
    "\n",
    "@Language.component(\"animal_component\")\n",
    "def animal_component(doc):\n",
    "    # Create a Span for each match and assign the label 'ANIMAL'\n",
    "    # and overwrite the doc.ents with the matched spans\n",
    "    doc.ents = [Span(doc, start, end, label='ANIMAL')\n",
    "                for match_id, start, end in matcher(doc)]\n",
    "    return doc\n",
    "    \n",
    "# Add the component to the pipeline after the 'ner' component \n",
    "nlp.add_pipe(\"animal_component\", after=\"ner\")\n",
    "\n",
    "# Process the text and print the text and label for the doc.ents\n",
    "doc = nlp(\"I have a cat and a Golden Retriever\")\n",
    "print([(ent.text, ent.label_) for ent in doc.ents])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21623fbb",
   "metadata": {},
   "source": [
    "### Examples using extensions\n",
    "\n",
    "Extensions allow additions of custom metadata to documents, tokens and spans.\n",
    "\n",
    "Accessible via the ._ property - \n",
    "- doc._.title = 'My document'\n",
    "- token._.is_color = True\n",
    "- span._.has_color = False\n",
    "\n",
    "Registered using the set_extension method -\n",
    "\n",
    "Doc.set_extension('title', default=None)\n",
    "\n",
    "Types \n",
    "- Attribute: set a default value that can be overwritten\n",
    "- Property: allow definition of a getter and setter, getter is called when value is retrieved\n",
    "- Method: assign a function that becomes available as an object method, lets you pass args to the extension function\n",
    "\n",
    "Method example:\n",
    "\n",
    "from spacy.tokens import Doc\n",
    "\n",
    "def has_token(doc, token_text):\n",
    "\n",
    "  in_doc = token_text in [token.text for token in doc]\n",
    "  \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "a043e18f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reversed: llA\n",
      "reversed: snoitazilareneg\n",
      "reversed: era\n",
      "reversed: eslaf\n",
      "reversed: ,\n",
      "reversed: gnidulcni\n",
      "reversed: siht\n",
      "reversed: eno\n",
      "reversed: .\n"
     ]
    }
   ],
   "source": [
    "from spacy.tokens import Token\n",
    "\n",
    "# Define the getter function that takes a token and returns its reversed text\n",
    "def get_reversed(token):\n",
    "    return token.text[::-1]\n",
    "  \n",
    "# Register the Token property extension 'reversed' with the getter get_reversed\n",
    "Token.set_extension('reversed', getter=get_reversed)\n",
    "\n",
    "# Process the text and print the reversed attribute for each token\n",
    "doc = nlp(\"All generalizations are false, including this one.\")\n",
    "for token in doc:\n",
    "    print('reversed:', token._.reversed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb371b10",
   "metadata": {},
   "source": [
    "Wikipedia URL Extension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "3967b35b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_wikipedia_url(span):\n",
    "    # Get a Wikipedia URL if the span has one of the labels\n",
    "    if span.label_ in ('PERSON', 'ORG', 'GPE', 'LOCATION'):\n",
    "        entity_text = span.text.replace(' ', '_')\n",
    "        return \"https://en.wikipedia.org/w/index.php?search=\" + entity_text\n",
    "\n",
    "# Set the Span extension wikipedia_url using get getter get_wikipedia_url\n",
    "Span.set_extension('wikipedia_url', getter=get_wikipedia_url, force=True)\n",
    "\n",
    "doc = nlp(\"In over fifty years from his very first recordings right through to his last album, David Bowie was at the vanguard of contemporary culture.\")\n",
    "for ent in doc.ents:\n",
    "    # Print the text and Wikipedia URL of the entity\n",
    "    print(ent.text, ent._.wikipedia_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c27317a1",
   "metadata": {},
   "source": [
    "Example of a Countries Component - doesn't work because no matcher is defined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "0740d9e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "@Language.component(\"countries_component\")\n",
    "def countries_component(doc):\n",
    "    # Create an entity Span with the label 'GPE' for all matches\n",
    "    doc.ents = [Span(doc, start, end, label='GPE')\n",
    "                for match_id, start, end in matcher(doc)]\n",
    "    return doc\n",
    "\n",
    "# Add the component to the pipeline\n",
    "nlp.add_pipe('countries_component')\n",
    "\n",
    "# Register capital and getter that looks up the span text in country capitals\n",
    "Span.set_extension('capital', getter=lambda span: capitals.get(span.text))\n",
    "\n",
    "# Process the text and print the entity text, label and capital attributes\n",
    "doc = nlp(\"Czech Republic may help Slovakia protect its airspace\")\n",
    "print([(ent.text, ent.label_, ent._.capital) for ent in doc.ents])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "730b4f8b",
   "metadata": {},
   "source": [
    "### Optimization\n",
    "\n",
    "Using nlp.pipe to process many texts at once"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "af8d4eda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['important', 'good', 'dark', 'unnatural']\n",
      "['dirty', 'great']\n",
      "['more', 'farthest']\n",
      "['exciting']\n",
      "['hazel']\n",
      "['beautiful']\n"
     ]
    }
   ],
   "source": [
    "TEXTS = [\"N-nothing important. That is, I heard a good deal about a ring, and a dark lord, and something about the end of the world, but please, Mr. Gandalf, sir, don't hurt me. Don't turn me into anything... unnatural.\",\n",
    "         \"There's a dirty great root sticking into my back!\", \n",
    "         \"If I take one more step, it'll be the farthest from home I've every been.\",\n",
    "         \"I wonder if we'll ever be put into exciting stories or tales?\",\n",
    "         \"It'll be spring soon, and the orchards will be in blossom. The birds will be nesting in the hazel thicket.\",\n",
    "         \"Rosie Cotton, dancing...she had beautiful flowers in her hair.\"]\n",
    "\n",
    "docs = list(nlp.pipe(TEXTS))\n",
    "for doc in docs:\n",
    "    print([token.text for token in doc if token.pos_ == 'ADJ'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d4f880c",
   "metadata": {},
   "source": [
    "### Customization\n",
    "\n",
    "Loading custom document attributes values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "2b51f0d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One morning, when Gregor Samsa woke from troubled dreams, he found himself transformed in his bed into a horrible vermin. \n",
      " — 'Metamorphosis' by Franz Kafka \n",
      "\n",
      "I know not all that may be coming, but be it what it will, I'll go to it laughing. \n",
      " — 'Moby-Dick or, The Whale' by Herman Melville \n",
      "\n",
      "It was the best of times, it was the worst of times. \n",
      " — 'A Tale of Two Cities' by Charles Dickens \n",
      "\n",
      "The only people for me are the mad ones, the ones who are mad to live, mad to talk, mad to be saved, desirous of everything at the same time, the ones who never yawn or say a commonplace thing, but burn, burn, burn like fabulous yellow roman candles exploding like spiders across the stars. \n",
      " — 'On the Road' by Jack Kerouac \n",
      "\n",
      "It was a bright cold day in April, and the clocks were striking thirteen. \n",
      " — '1984' by George Orwell \n",
      "\n",
      "Nowadays people know the price of everything and the value of nothing. \n",
      " — 'The Picture Of Dorian Gray' by Oscar Wilde \n",
      "\n"
     ]
    }
   ],
   "source": [
    "DATA = [('One morning, when Gregor Samsa woke from troubled dreams, he found himself transformed in his bed into a horrible vermin.',\n",
    "  {'author': 'Franz Kafka', 'book': 'Metamorphosis'}),\n",
    " (\"I know not all that may be coming, but be it what it will, I'll go to it laughing.\",\n",
    "  {'author': 'Herman Melville', 'book': 'Moby-Dick or, The Whale'}),\n",
    " ('It was the best of times, it was the worst of times.',\n",
    "  {'author': 'Charles Dickens', 'book': 'A Tale of Two Cities'}),\n",
    " ('The only people for me are the mad ones, the ones who are mad to live, mad to talk, mad to be saved, desirous of everything at the same time, the ones who never yawn or say a commonplace thing, but burn, burn, burn like fabulous yellow roman candles exploding like spiders across the stars.',\n",
    "  {'author': 'Jack Kerouac', 'book': 'On the Road'}),\n",
    " ('It was a bright cold day in April, and the clocks were striking thirteen.',\n",
    "  {'author': 'George Orwell', 'book': '1984'}),\n",
    " ('Nowadays people know the price of everything and the value of nothing.',\n",
    "  {'author': 'Oscar Wilde', 'book': 'The Picture Of Dorian Gray'})]\n",
    "\n",
    "from spacy.tokens import Doc\n",
    "Doc.set_extension('book', default=None, force=True)\n",
    "Doc.set_extension('author', default=None, force=True)\n",
    "\n",
    "for doc, context in nlp.pipe(DATA, as_tuples=True):\n",
    "    # Set the doc._.book and doc._.author attributes from the context\n",
    "    doc._.book = context['book']\n",
    "    doc._.author = context['author']\n",
    "    \n",
    "    # Print the text and custom attribute data\n",
    "    print(doc.text, '\\n', \"— '{}' by {}\".format(doc._.book, doc._.author), '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebc9cd4c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
